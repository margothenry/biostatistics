---
title: "Chapter4"
output: html_document
---

```{r}
library(tidyverse)
```
1.
```{r}
set.seed(1234)
Myst <- readRDS("~/Desktop/stat 4600/biostatistics/data/Myst.rds")
yvar = Myst$yvar
ggplot(tibble(yvar), aes(x = yvar)) + geom_histogram(binwidth=0.025)
str(yvar)
```

This assigns random probability of membership
pA = probability of coming from micture A
```{r}
pA = runif(length(yvar)) 
```
pB = complimentary probability

```{r}
pB = 1 - pA
```
E step: expected value of the log likelihood function.
M step: maximizing.

maxiter: Maximum number of iterations.

miniter: Minimum number of iterations.

minprior: Minimum prior probability of clusters, components falling below this threshold are removed during the iteration.

tolerance: The EM algorithm is stopped when the (relative) change of log-likelihood is smaller than tolerance.

```{r}
iter = 0
loglik = -Inf
delta = +Inf
tolerance = 1e-3
miniter = 50; maxiter = 1000
```

```{r}
while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {
  #E-STEP
  lambda = mean(pA)
  muA = weighted.mean(yvar, pA) 
  muB = weighted.mean(yvar, pB) 
  sdA = sqrt(weighted.mean((yvar - muA)^2, pA)) 
  sdB = sqrt(weighted.mean((yvar - muB)^2, pB)) 
  
  #M-STEP
  phiA = dnorm(yvar, mean = muA, sd = sdA) 
  phiB = dnorm(yvar, mean = muB, sd = sdB) 
  pA   = lambda * phiA
  pB   = (1 - lambda) * phiB
  
  ptot = pA + pB # E-Step (Normalizer)
  pA   = pA / ptot # E-Step (Post)
  pB   = pB / ptot # E-Step (Post)
  
  loglikOld = loglik
  loglik = sum(log(pA))
  delta = abs(loglikOld - loglik) #loglik used here. To calculate delta?
  iter = iter + 1
}
param = tibble(group = c("A","B"), mean = c(muA,muB), sd = c(sdA,sdB))
param
```
Check this out to better understand E-Step and M-Step (Maybe im just dumb)
http://tinyheero.github.io/2016/01/03/gmm-em.html#expectation-calculating-the-soft-labels-of-each-data-point-e-step

These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.

compare with mixtools package.

2
```{r}
library("vcd")
lambda = rgamma(10000, shape = 10, rate = 3/2)
gp = rpois(length(lambda), lambda = lambda)
ggplot(tibble(x = gp), aes(x = x)) +
  geom_histogram(bins = 100, fill= "purple")

ofit = goodfit(gp, "nbinomial")
plot(ofit, xlab = "")
ofit$par

plot(ofit, xlab = "")
```
simbat : the output of the lapply loop is a list of tibbles, one for each value of lambdas. 
```{r}
lambdas = seq(100, 900, by = 100)
simdat = lapply(lambdas, function(l)
  tibble(y = rpois(n = 40, lambda=l), lambda = l)
) %>% bind_rows
```

```{r}
library("ggbeeswarm")
#using the lambda we found in it$par
ggplot(simdat, aes(x = lambda, y = y)) +
  geom_beeswarm(alpha = 0.6, color = "purple")
ggplot(simdat, aes(x = lambda, y = sqrt(y))) +
  geom_beeswarm(alpha = 0.6, color = "purple")
#nevermind we dont do this, but it looks cool
```

```{r}
shape = ofit$par[[1]] 
prob = ofit$par[[2]]
test1 <- rgamma(10000, shape = shape, rate = prob )
test2 <- rgamma(10000, shape = 10, rate = 3/2)
qqplot( y = test1, x = test2 )

test1 <- rgamma(10000, shape = shape, rate = prob )
test3 <- rgamma(10000, shape = 10, rate = 1/3)
qqplot( y = test1, x = test3 )
```

3
```{r}
library("flexmix")
data(NPreg)
names(NPreg)
```
A)
```{r}
NPreg %>% ggplot(aes(x = x, y = yn)) +
  geom_point()
NPreg %>% ggplot(aes(x = x, y = yp)) +
  geom_point()
NPreg %>% ggplot(aes(x = x, y = yb)) +
  geom_point()
#going to use yn as my y variable, cause its the only one with a decent spread

#just having fun now
NPreg %>% ggplot(aes(x = x, y = id1)) +
  geom_point()
NPreg %>% ggplot(aes(x = x, y = id2)) +
  geom_point()
NPreg %>% ggplot(aes(x = x, y = class)) +
  geom_point()
```

```{r}
ggplot(NPreg %>% filter(class == "1"), aes(x = x)) +
  geom_histogram(binwidth = 0.5, fill = "forestgreen")

ggplot(NPreg %>% filter(class == "2"), aes(x = x)) +
  geom_histogram(binwidth = 0.5, fill = "blue")

# I think they were plotted by classes, class 1 and class 2
```

B) Fit a two component mixture model using the commands
```{r}
m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)
m1
#not too sure what Fit a two component mixture model using the commands means, i think maybe something like this? or do i have to graph two histoograms one ontop of another. 
matplot(NPreg$x, fitted(m1), pch = 16, type="p")
points(NPreg$x, NPreg$yn)
```

C)
```{r}
table <- select(NPreg, class) 
table$cluster_membership = ''
table$cluster_membership =  m1@cluster
table$truth_table = table$class == table$cluster_membership
```

D)
```{r}
NPreg %>% ggplot(aes(x = x, y = yn)) +
  geom_point(colour = table$cluster_membership)
```
4 
Some possible papers:
# https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00756.x 
# http://users.umiacs.umd.edu/~hal/docs/daume08ihfrm.pdf 
# http://www.stats.ox.ac.uk/~teh/research/npbayes/HelTehGor2009a.pdf 
# https://ieeexplore.ieee.org/abstract/document/4407680
# https://projecteuclid.org/euclid.ba/1340370725